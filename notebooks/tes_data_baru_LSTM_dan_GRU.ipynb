{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cek data"
      ],
      "metadata": {
        "id": "GRln5YIcRJhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import joblib\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def preprocess_new_data(new_data):\n",
        "    import pandas as pd\n",
        "    import re\n",
        "\n",
        "    # Mengatur parameter stop_words dan stemmer\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = str(text)\n",
        "        text = re.sub(r'\\d+', ' ', text)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    new_df = pd.DataFrame([new_data])\n",
        "    new_df['Note'].fillna('', inplace=True)\n",
        "\n",
        "    # Preprocessing teks\n",
        "    new_df['Note'] = new_df['Note'].apply(clean_text)\n",
        "\n",
        "    activity_weights = {\n",
        "        'TASK': 1,\n",
        "        'CALL': 2,\n",
        "        'DEADLINE': 1,\n",
        "        'EMAIL': 1,\n",
        "        'MEETING': 3\n",
        "    }\n",
        "    new_df['Activity Score'] = new_df.apply(lambda row: activity_weights.get(row['Type Activity'], 0), axis=1)\n",
        "\n",
        "    return new_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWGFoqYzRJT6",
        "outputId": "a1bc5a35-f7a8-4587-d725-9b74e26ca69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_new_features(new_df, tokenizer, max_sequence_length):\n",
        "    # Tokenisasi dan padding\n",
        "    sequences = tokenizer.texts_to_sequences(new_df['Note'])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "    X_new = np.hstack((new_df[['Activity Score']].values, padded_sequences))\n",
        "    return X_new"
      ],
      "metadata": {
        "id": "b1-Lh7Y88eG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_data(new_data, model, tokenizer, max_sequence_length):\n",
        "    new_df = preprocess_new_data(new_data)\n",
        "    X_new = extract_new_features(new_df, tokenizer, max_sequence_length)\n",
        "\n",
        "    y_pred = model.predict(X_new)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Mapping angka ke label asli\n",
        "    label_mapping = {4: 'LOST', 0: 'COLD', 1: 'WARM', 2: 'HOT', 3: 'DEAL'}\n",
        "    #label_mapping = {0: 'LOST', 1: 'COLD', 2: 'WARM', 3: 'HOT', 4: 'DEAL'}\n",
        "    predicted_label = label_mapping[y_pred_class[0]]\n",
        "\n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "MI5NKp1m8fjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Data baru\n",
        "    new_data = {\n",
        "        'Deal Name': 'New Deal 1',\n",
        "        'Type Activity': 'TASK',\n",
        "        'Note': 'menunggu informasi selanjutnya'\n",
        "    }\n",
        "\n",
        "    # Memuat model yang telah disimpan\n",
        "    model = tf.keras.models.load_model('modelLSTMUpdateTry.h5')\n",
        "    tokenizer = joblib.load('tokenizerUpdate (2).pkl')\n",
        "    max_sequence_length = 100\n",
        "\n",
        "    predicted_label = predict_new_data(new_data, model, tokenizer, max_sequence_length)\n",
        "\n",
        "    print(\"Predicted Label for the new deal:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nimarg0ORZT7",
        "outputId": "2bdfe6a0-2857-45a5-c11e-c5353a1f1a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
            "Predicted Label for the new deal: WARM\n"
          ]
        }
      ]
    }
  ]
}